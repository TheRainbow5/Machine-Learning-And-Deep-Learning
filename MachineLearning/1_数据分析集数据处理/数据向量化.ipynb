{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据向量化\n",
    "* CountVectorizer\n",
    "* TF-IDF\n",
    "* TF-idfVectorizer\n",
    "\n",
    "文本向量化的方法主要分为**离散表示**和**分布式表示。**\n",
    "\n",
    "## 1离散表示\n",
    "    一种基于规则和统计的向量化方式，常用的方法包括词集模型和词袋模型，都是基于词之间保持独立性、没有关联为前提，将所有文本中单词形成一个字典，然后根据字典来统计单词出现频数，不同的是：\n",
    "### 1.1词集模型：\n",
    "    例如One-Hot Representation，只要单个文本中单词出现在字典中，就将其置为1，不管出现多少次\n",
    "\n",
    "### 1.2词袋模型：\n",
    "    只要单个文本中单词出现在字典中，就将其向量值加1，出现多少次就加多少次\n",
    "对于句子或篇章而言，常用的离散表示方法是词袋模型。词袋模型以One-Hot为基础，忽略词表中词的顺序和语法关系，通过记录词表中的每一个词在该文本中出现的频次来表示该词在文本中的重要程度，解决了 One-Hot 未能考虑词频的问题。\n",
    "* 优点：方法简单，当语料充足时，处理简单的问题如文本分类，其效果比较好。\n",
    "* 缺点：数据稀疏、维度大，且**不能很好地展示词与词之间的相似关系。**\n",
    "\n",
    "其基本的特点是**忽略了文本信息中的语序信息和语境信息**，仅将其反映为若干维度的独立概念，这种情况有着因为模型本身原因而无法解决的问题，比如主语和宾语的顺序问题，词袋模型天然无法理解诸如“我为你鼓掌”和“你为我鼓掌”两个语句之间的区别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer（词袋模型）（线性模型）\n",
    "    CountVectorizer 根据文本构建出一个词表，词表中包含了所有文本中的单词，每一个词汇对应其出现的顺序，构建出的词向量的每一维都代表这一维对应单词出现的频次，这些词向量组成的矩阵称为频次矩阵。\n",
    "* 缺点：CountVectorizer**只能表达词在当前文本中的重要性，无法表示该词在整个文档集合中的重要程度。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 5)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t2\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 8)\t1\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 2]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "#多少行数据=矩阵有多少行\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the this second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "cv=CountVectorizer()  #初始化\n",
    "#计算词语出现的次数\n",
    "x=cv.fit_transform(corpus)\n",
    "print(x)\n",
    "#获取文本的关键字\n",
    "word=cv.get_feature_names()\n",
    "print(word)\n",
    "#查看词频结果\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF（词袋模型）\n",
    "    TF-IDF（词频-逆文档频率法，Term Frequency–Inverse Document Frequency）作为一种加权方法，在词袋模型的基础上对词出现的频次赋予TF-IDF权值，对词袋模型进行修正，进而表示该词在文档集合中的重要程度。\n",
    "在利用TF-IDF进行特征提取时，若词α在某篇文档中出现频率较高且在其他文档中出现频率较低时，则认为α可以代表该文档的特征，具有较好的分类能力，那么α作为特征被提取出来。\n",
    "### 应用\n",
    "* 搜索引擎\n",
    "* 关键词提取\n",
    "* 文本相似性\n",
    "* 文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-idfVectorizer（线性模型）（不适用于朴素贝叶斯）\n",
    "    如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。可以把原始文本转化为tf-idf的特征矩阵，从而为后续的文本相似度计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 是 一条 天狗 呀', '我 把 月 来 吞 了 ，', '我 把 日来 吞 了 ，', '我 把 一切 的 星球 来 吞 了 ，', '我 把 全宇宙 来 吞 了 。', '我 便是 我 了']\n",
      "{'一条': 1, '天狗': 4, '日来': 5, '一切': 0, '星球': 6, '全宇宙': 3, '便是': 2}\n",
      "[[0.         0.70710678 0.         0.         0.70710678 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.        ]\n",
      " [0.70710678 0.         0.         0.         0.         0.\n",
      "  0.70710678]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#对付中文\n",
    "'''中文不比英文，词语之间有着空格的自然分割，所以我们首先要进行分词处理，再把它转化为与上面的document类似的格式。这里采用著名的中文分词库jieba进行分词'''\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = \"\"\"我是一条天狗呀\n",
    "我把月来吞了，\n",
    "我把日来吞了，\n",
    "我把一切的星球来吞了，\n",
    "我把全宇宙来吞了。\n",
    "我便是我了\"\"\"\n",
    "sentences = text.split()\n",
    "sent_words = [list(jieba.cut(sent0)) for sent0 in sentences]\n",
    "document = [\" \".join(sent0) for sent0 in sent_words]\n",
    "print(document)\n",
    "\n",
    "#建立模型\n",
    "tf=TfidfVectorizer()\n",
    "data=tf.fit_transform(document) \n",
    "print(tf.vocabulary_)   #查看词典\n",
    "print(data.toarray())   #转化为矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "没有错误，但有一个小问题，就是单字的词语，如“我”、“吞”、“呀”等词语在我们的词汇表中怎么都不见了呢？为了处理一些特殊的问题，让我们深入其中的一些参数。\n",
    "\n",
    "发现单字的问题是token_pattern这个参数搞的鬼。它的默认值只匹配长度≥2的单词，就像其实开头的例子中的'I'也被忽略了一样，一般来说，长度为1的单词在英文中一般是无足轻重的，但在中文里，就可能有一些很重要的单字词，所以修改如下：\n",
    "\n",
    "token_pattern这个参数使用正则表达式来分词，其默认参数为r\"(?u)\\b\\w\\w+\\b\"，其中的两个\\w决定了其匹配长度至少为2的单词，所以这边减到1个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'我': 8, '是': 12, '一条': 1, '天狗': 7, '呀': 6, '把': 9, '月': 13, '来': 14, '吞': 5, '了': 2, '日来': 10, '一切': 0, '的': 15, '星球': 11, '全宇宙': 4, '便是': 3}\n"
     ]
    }
   ],
   "source": [
    "tfidf_model2 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(document)\n",
    "print(tfidf_model2.vocabulary_)\n",
    "# {'我': 8, '是': 12, '一条': 1, '天狗': 7, '呀': 6, '把': 9, '月': 13, '来': 14, '吞': 5, '了': 2, '日来': 10, '一切': 0, '的': 15, '星球': 11, '全宇宙': 4, '便是': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop_words: list类型\n",
    "    直接过滤指定的停用词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'我': 8, '一条': 1, '天狗': 7, '呀': 6, '把': 9, '月': 12, '来': 13, '吞': 5, '了': 2, '日来': 10, '一切': 0, '星球': 11, '全宇宙': 4, '便是': 3}\n"
     ]
    }
   ],
   "source": [
    "tfidf_model4 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",stop_words=[\"是\", \"的\"]).fit(document)\n",
    "print(tfidf_model4.vocabulary_)\n",
    "# {'一条': 1, '天狗': 5, '呀': 4, '月': 8, '来': 9, '日来': 6, '一切': 0, '星球': 7, '全宇宙': 3, '便是': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary: dict类型\n",
    "      这一参数的使用有时能帮助我们专注于一些词语，比如我对本诗中表达感情的一些特定词语（甚至标点符号）感兴趣，就可以设定这一参数，只考虑他们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'我': 0, '呀': 1, '!': 2}\n",
      "[[0.40572238 0.91399636 0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_model5 = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",vocabulary={\"我\":0, \"呀\":1,\"!\":2}).fit(document)\n",
    "print(tfidf_model5.vocabulary_)\n",
    "print(tfidf_model5.transform(document).todense())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f03fb8992b0e5aa6a67cd56eb653950a01f214acb97d94a2178dd1aa47d1bcf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
