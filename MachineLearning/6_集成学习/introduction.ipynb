{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 集成学习\n",
    "* boosting\n",
    "* bagging\n",
    "## 1 什么是集成学习\n",
    "![](https://img-blog.csdnimg.cn/a5b56caf2eaf46498833c195ad6f31ce.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_13,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "集成学习通过建⽴⼏个模型来解决单⼀预测问题。它的⼯作原理是**⽣成多个分类器/模型，各⾃独⽴地学习和作出预测**。这些预测最后结合成组合预测，因此优于任何⼀个单分类的做出预测。\n",
    "## 2 机器学习的俩个核心任务\n",
    "* 如何优化训练数据 —> 主要⽤于解决⽋拟合问题\n",
    "* 如何提升泛化性能 —> 主要⽤于解决过拟合问题\n",
    "## 3 集成学习中的Boosting和Bagging\n",
    "![](https://img-blog.csdnimg.cn/32669202f74f46c88dfc297c842237ce.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_14,color_FFFFFF,t_70,g_se,x_16)\n",
    "（只要单分类器的表现不错，集成学习的结果总是优于单分类器）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Bagging和随机森林\n",
    "* Bagging集成原理\n",
    "* 随机森林构造过程\n",
    "* 什么是包外估计\n",
    "* RandomForestClassifier的使⽤\n",
    "* baggind集成的优点\n",
    "## 1 Bagging集成原理\n",
    "目标：把下⾯的圈和⽅块进⾏分类。\n",
    "![](https://img-blog.csdnimg.cn/a558687825204425be4152bac1fcb535.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "### 1.1实现过程\n",
    "* 1、采样不同的数据\n",
    "![](https://img-blog.csdnimg.cn/50c6947e423f4ff0827eea1256cd6e32.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "（随机采样）\n",
    "* 2、训练分类器\n",
    "![](https://img-blog.csdnimg.cn/6d9dddeb132e4dae9bace16694137b0a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "* 3、平权投票，获取最终结果\n",
    "![](https://img-blog.csdnimg.cn/84bbe525c5f144989bcf1b0bb867b017.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "* 4、实现过程小结\n",
    "![](https://img-blog.csdnimg.cn/f0abc2d97fa443d59101c1304b14fbe4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "## 2 随机森林构造过程\n",
    "在机器学习中，随机森林是⼀个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数⽽定。**随机森林 = Bagging + 决策树**\n",
    "![](https://img-blog.csdnimg.cn/b93c4e3f8d004dd6a209eb17b8251937.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "（例如：如果你训练了5个树, 其中有4个树的结果是True, 1个树的结果是False, 那么最终投票结果就是True）\n",
    "### 2.1随机森林的关键步骤\n",
    "* ⼀次随机选出⼀个样本，有放回的抽样，重复N次(有可能出现重复的样本)\n",
    "* 随机去选出m个特征, m <<M，建⽴决策树\n",
    "#### 思考\n",
    "* 为什么要随机抽取训练集？\n",
    "    * 如果不进⾏随机抽样，每棵树的训练集都⼀样，那么最终训练出的树分类结果也是完全⼀样的。\n",
    "* 为什么要有放回地抽样\n",
    "    * 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，**都是没有交集的**，这样每棵树都是“有偏的”，都是绝对“⽚⾯的”(当然这样说可能不对)，也就是说每棵树训练出来都是有很⼤的差异的；⽽随机森林最后分类取决于多棵树(弱分类器)的投票表决。\n",
    "\n",
    "## 3 包外估计（Out-of-Bag Estimate）\n",
    "### 3.1 包外估计的定义\n",
    "在随机森林的Bagging过程中，对于一颗训练出来的决策树Gt,与数据集D有如下关系。\n",
    "![](https://img-blog.csdnimg.cn/c98c6c32995e43939dd36541391f7b93.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "**对于星号部分的数据，即是没有选择到的数据，称之为 Out-of-bag(OOB)数据**，当数据⾜够多，对于任意⼀组数据 (xn，yn)是包外数据的概率为：\n",
    "![](https://img-blog.csdnimg.cn/06039adf6bb64dfa8720477ccc58910f.png)\n",
    "\n",
    "由于基分类器是构建在训练样本的⾃助抽样集上的，只有约 63.2％ 原样本集出现在中，⽽剩余的 36.8％ 的数据作为包外数据，可以⽤于基分类器的验证集。\n",
    "\n",
    "经验证，包外估计是对集成分类器泛化误差的⽆偏估计。在随机森林算法中数据集属性的重要性、分类器集强度和分类器间相关性计算都依赖于包外数据。\n",
    "### 3.2 包外估计的用途\n",
    "* 当基学习器是决策树时，可使⽤包外样本来辅助剪枝 ，或⽤于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；\n",
    "* 当基学习器是神经⽹络时，可使⽤包外样本来辅助早期停⽌以减⼩过拟合 。\n",
    "\n",
    "## 4 Bagging集成优点\n",
    "Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习\n",
    "\n",
    "⽅法经过上⾯⽅式组成的集成学习⽅法:\n",
    "* 均可在原有算法上提⾼约2%左右的泛化正确率\n",
    "* 简单, ⽅便, 通⽤\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
