{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树（离散型数据）\n",
    "* 它是一种监督学习算法\n",
    "* 主要用于分类问问题\n",
    "* 数据：可分类、连续输入和输出变量\n",
    "![](https://images2017.cnblogs.com/blog/1244340/201710/1244340-20171008111205246-142180917.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.熵\n",
    "\n",
    "物理学上，熵（entropy）是“混乱”程度的度量。\n",
    "![](https://img-blog.csdnimg.cn/535095dcdde543408053e89e6874bd34.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_14,color_FFFFFF,t_70,g_se,x_16)\n",
    "（上图解释：系统越有序，熵值越低；反之，越高）\n",
    "\n",
    "### 1.1信息熵（information entropy）\n",
    "    信息熵是度量样本集合最常用的一种指标。\n",
    "\n",
    "假定当前样本集合 D 中第 k 类样本所占的⽐例为 p (k = 1, 2,. . . , |y|) \n",
    "\n",
    "p =Ck/D, D为样本的所有数量，Ck为第k类样本的数量。\n",
    "\n",
    "则 D的信息熵定义为(（log是以2为底，lg是以10为底）:\n",
    "![](https://img-blog.csdnimg.cn/528663b9a0644224901f45d36aadfc16.png)\n",
    "（上图解释：Ent(D) 的值越⼩，则 D 的纯度越⾼。）\n",
    "\n",
    "### 1.2条件熵\n",
    "\n",
    "假设样本集D包含离散属性a和b俩类。\n",
    "\n",
    "其中a属性有a1、a2、a3.....an（一共有n个样本），如果用a来对样本集D进行划分，则会产生n个分支结点。\n",
    "\n",
    "其中第n个分支结点包含了D中所有在属性a上取值为an的样本，记作Dn。我们可以根据前面给出的信息熵的公式计算出Dn的信息熵；再考虑到不同的分支结点所包含的样本数不一样，给分支节点赋予**权重|Dn|/|D|（这就是条件熵）**\n",
    "\n",
    "即样本数量越多的分支结点的影响越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.信息增益（ID3决策树）\n",
    "备注：只能对离散属性的数据集构成决策树。\n",
    "### 2.1概念\n",
    "    以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越⼤，样本的不确定性就越⼤。\n",
    "    因此可以使⽤划分前后集合熵的差值来衡量使⽤当前特征对于样本集合D划分效果的好坏。\n",
    "\n",
    "* 信息增益=信息熵-条件熵\n",
    "\n",
    "由第1节可知，特征a对训练数据集D的信息增益Gain(D,a),定义为集合D的信息熵Ent(D)与给定特征a条件下D的信息条件熵Ent(D∣a)之\n",
    "差，即公式为：\n",
    "![](https://img-blog.csdnimg.cn/54fd22fb105c47b4b5a91b8a0c2b2426.png)\n",
    "* 信息熵的计算：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/a41adc0c0101497ca31e4be7b6f7ccce.png)\n",
    "* 条件熵的计算：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/40a6c12b32f24bae9d8d358a284e54b2.png)\n",
    "\n",
    "其中：   \n",
    "\n",
    "Dv表示a属性中第v个分⽀节点包含的样本数。\n",
    "\n",
    "Ckv表示a属性中第v个分⽀节点包含的样本数中，第k个类别下包含的样本数。\n",
    "\n",
    "⼀般⽽⾔，**信息增益越⼤，则意味着使⽤属性 a 来进⾏划分所获得的\"纯度提升\"越⼤**。因此，我们可⽤信息增益来进⾏决策树的划分属性选择，著名的 ID3 决策树学习算法 [Quinlan， 1986] 就是以信息增益为准则来选择划分属性。\n",
    "\n",
    "（注：每次删除一个特征值时，都要重新计算信息熵和条件）\n",
    "\n",
    "### 2.2案例\n",
    "如下图，第⼀列为论坛号码，第⼆列为性别，第三列为活跃度，最后⼀列⽤户是否流失。\n",
    "我们要解决⼀个问题：性别和活跃度两个特征，哪个对⽤户流失影响更⼤？\n",
    "![](https://img-blog.csdnimg.cn/133c1aeffbf347eeb65173111679bb91.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "（上图解释：Positive为正样本（已流失），Negative为负样本（未流失），下⾯的数值为不同划分下对应的⼈数。）\n",
    "通过计算信息增益解决这个问题。\n",
    "#### 1.计算整体信息熵\n",
    "![](https://img-blog.csdnimg.cn/a5caadb26bef469db8d7e40fdfdefa14.png)\n",
    "#### 2.属性=“性别”\n",
    "* 信息熵\n",
    "![](https://img-blog.csdnimg.cn/86b511728f484d85b83892115e4d344b.png)\n",
    "* 信息增益\n",
    "![](https://img-blog.csdnimg.cn/0f020335f4874ad5b3fc7e260141a51f.png)\n",
    "#### 3.属性=“活跃度”\n",
    "* 信息熵\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/0e085699be3141d8a08767f9d8b92a7a.png)\n",
    "* 信息增益\n",
    "![](https://img-blog.csdnimg.cn/ce2cc1b97317425ab353e8c3c95e8e78.png)\n",
    "\n",
    "通过信息增益对比，活跃度的信息增益⽐性别的信息增益⼤，也就是说，活跃度对⽤户流失的影响⽐性别⼤。在做特征选择或者数据分析的\n",
    "时候，我们应该重点考察活跃度这个指标。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.信息增益率（C4.5决策树）\n",
    "备注：优化后，解决了ID3分支过程偏向选择值较多的属性。\n",
    "### 3.1概念\n",
    "在上⾯的介绍中，我们有意忽略了\"编号\"这⼀列.若把\"编号\"也作为⼀个候选划分属性，则根据信息增益公式可计算出它的信息增益为 0.9182，远⼤于其他候选划分属性。\n",
    "\n",
    "（计算每个属性的信息熵过程中,我们发现,该属性的值为0, 也就是其信息增益为0.9182. 但是很明显这么分类,最后出现的结果不具有泛化效果（预测效果）.⽆法对新样本进⾏有效预测。）\n",
    "\n",
    "实际上，**信息增益准则对可取值数⽬较多的属性有所偏好**，为减少这种偏好可能带来的不利影响，**著名的 C4.5 决策树算法 [Quinlan， 1993J 不直接使⽤信息增益，⽽是使⽤\"增益率\" (gain ratio) 来选择最优划分属性。**\n",
    "\n",
    "### 3.2增益率\n",
    "增益率是⽤前⾯的信息增益Gain(D, a)和**属性a对应的\"固有值\"(intrinsic value)**[Quinlan , 1993]的⽐值来共同定义的。\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/3bd1ad04fb724169ae4d8a01a8f15a68.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_7,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "（上图解释：属性 a 的可能取值数⽬越多(即 V 越⼤)，则 IV(a) 的值通常会越⼤。）\n",
    "\n",
    "### 3.3属性分类信息度量\n",
    "* 内在信息：⽤分裂信息度量来考虑某种属性进⾏分裂时分⽀的**数量信息和尺⼨信息**\n",
    "* 信息增益率⽤信息增益/内在信息，会导致属性的重要性随着内在信息的增⼤⽽减⼩（也就是\n",
    "说，如果这个属性本身不确定性就很⼤，那我就越不倾向于选取它），这样算是对单纯⽤信息增益有所补偿。\n",
    "![](https://img-blog.csdnimg.cn/88a4c3b93dcb48e5bc54cc872b477278.png)\n",
    "### 3.4案例\n",
    "![](https://img-blog.csdnimg.cn/133c1aeffbf347eeb65173111679bb91.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 计算类别信息熵\n",
    "* 计算性别属性的信息熵(性别、活跃度)\n",
    "* 计算活跃度的信息增益(性别、活跃度)\n",
    "#### 1、属性分类信息度量\n",
    "![](https://img-blog.csdnimg.cn/88a4c3b93dcb48e5bc54cc872b477278.png)\n",
    "#### 2、计算信息增益率\n",
    "![](https://img-blog.csdnimg.cn/0ffb718d326e41f6989c353e50852b23.png)\n",
    "#### 3、总结\n",
    "活跃度的信息增益率更⾼⼀些，所以在构建决策树的时候，优先选择\n",
    "通过这种⽅式，在选取节点的过程中，我们可以降低取值较多的属性的选取偏好。\n",
    "\n",
    "### 3.5案例二\n",
    "如下图，第⼀列为天⽓，第⼆列为温度，第三列为湿度，第四列为⻛速，最后⼀列该活动是否进⾏。\n",
    "我们要解决：根据下⾯表格数据，判断在对应天⽓下，活动是否会进⾏？\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/eafa8535ffa84c26ba3c5f82367a151d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_18,color_FFFFFF,t_70,g_se,x_16)\n",
    "![](https://img-blog.csdnimg.cn/4952032c341b482da7b863c7ad6bacc3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_18,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "（上图解释：属性集合A={ 天⽓，温度，湿度，⻛速}， 类别标签有两个，类别集合L={进⾏，取消}。）\n",
    "\n",
    "* 计算类别信息熵：\n",
    "    类别信息熵表示的是所有样本中各种类别出现的不确定性之和。根据熵的概念，熵越⼤，不确定性就越⼤，把事情搞清\n",
    "楚所需要的信息量就越多。\n",
    "![](https://img-blog.csdnimg.cn/8f50cfeaaec44de69b247e0935c8fc12.png)\n",
    "* 计算每个属性的信息熵（条件熵）\n",
    "    他表示的是在某种属性的条件下，各种类别出现的不确定性之和。属性的信息熵越⼤，表示这个属性中拥有的样本类别越不“纯”。\n",
    "    \n",
    "![](https://img-blog.csdnimg.cn/0983a23ac5784678a30edf58a8c8328d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_18,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 计算信息增益（ID3算法的特征选择指标）\n",
    "    信息增益的 = 熵 - 条件熵，在这⾥就是 类别信息熵 - 属性信息熵，它表示的是信息不确定性减少的程度。\n",
    "![](https://img-blog.csdnimg.cn/fec20bbcee7346a8bb477b50d5dbaf01.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_10,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "（假设我们把上⾯表格1的数据前⾯添加⼀列为\"编号\",取值(1--14). 若把\"编号\"也作为⼀个候选划分属性,则根据前⾯步骤: 计算每个属性的信息熵过程中,我们发现,该属性的值为0, 也就是其信息增益为0.940. 但是很明显这么分类,最后出现的结果不具有泛化效果.此时根据信息增益就⽆法选择出有效分类特征。所以，C4.5选择使⽤信息增益率对ID3进⾏改进。）\n",
    "* 计算属性分类信息度量\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/fce0201adff04b50a894a8fe056c7041.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_12,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "* 计算信息增益率\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/fec20bbcee7346a8bb477b50d5dbaf01.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_10,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "* 选择结点\n",
    "天⽓的信息增益率最⾼，选择天⽓为分裂属性。发现分裂了之后，天⽓是“阴”的条件下，类别是”纯“的，所以把它定义为叶⼦节点，选择不“纯”的结点继续分裂（如下图）。\n",
    "![](https://img-blog.csdnimg.cn/d57b414f9f354a1899d6278f5608649b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "在⼦结点当中重复过程1~5，直到所有的叶⼦结点⾜够\"纯\"。\n",
    "* 总结C4.5算法流程\n",
    "while(当前节点\"不纯\")：\n",
    "* 1.计算当前节点的类别熵(以类别取值计算)\n",
    "* 2.计算当前阶段的属性熵(按照属性取值吓得类别取值计算)\n",
    "* 3.计算信息增益\n",
    "* 4.计算各个属性的分裂信息度量\n",
    "* 5.计算各个属性的信息增益率\n",
    "end while\n",
    "当前阶段设置为叶⼦节点\n",
    "\n",
    "## 3.6为什么使用C4.5要好？\n",
    "### 1.用信息增益率来选择属性\n",
    "克服了用信息增益来选择属性是偏向选择值多的属性的不足。\n",
    "### 2.采用了一种后剪枝方法\n",
    "避免树的⾼度⽆节制的增⻓，避免过度拟合数据。\n",
    "### 3.对于缺失值的处理\n",
    "* 方法1：\n",
    "在某些情况下，可供使⽤的数据可能缺少某些属性的值。假如〈x，c(x)〉是样本集S中的⼀个训练实例，但是其属性A的值A(x)未知。处理缺少属性值的⼀种策略是赋给它结点n所对应的训练实例中该**属性的最常⻅值**。\n",
    "* 方法2：\n",
    "为A的每个可能值赋予⼀个概率。\n",
    "\n",
    "例如，给定⼀个布尔属性A，如果结点n包含6个已知A=1和4个A=0的实例，那么A(x)=1的概率是0.6，⽽A(x)=0的概率是0.4。于是，实例x的60%被分配到A=1的分⽀，40%被分配到另⼀个分⽀。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、基尼值和基尼指数（CART算法）\n",
    "    CART决策树使用的是“基尼指数”来选择划分属性。\n",
    "### 4.1基尼值Gini(D)\n",
    "    从数据集D中随机抽取两个样本，其类别标记不⼀致的概率。故，Gini（D）值越⼩，数据集D的纯度越⾼。\n",
    "![](https://img-blog.csdnimg.cn/3a8130c8033348e58cb351849197a366.png)\n",
    "（上图解释：PK=Ck/D，D为样本的所有数量，Ck第k类样本的数量。）\n",
    "### 4.2基尼指数Gini_index(D)\n",
    "    ⼀般，选择使划分后基尼系数最⼩的属性作为最优化分属性。\n",
    "![](https://img-blog.csdnimg.cn/4338413672ff410d8dcefb925b19f905.png)\n",
    "### 4.3案例\n",
    "![](https://img-blog.csdnimg.cn/32fda79e17ff4c13a39aed94854e4f7f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_18,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 1、对数据集⾮序列标号属性{是否有房，婚姻状况，年收⼊}分别计算它们的Gini指数，取Gini指数最⼩的属性作为决策树的根节点属性。\n",
    "* 2、根节点的Gini值为：\n",
    "    * ![](https://img-blog.csdnimg.cn/39d6f4bd7b6e456886179af371e73a0c.png)\n",
    "* 3、当根据是否有房来进⾏划分时，Gini指数计算过程为：\n",
    "    * ![](https://img-blog.csdnimg.cn/bda82dd2c7694f119936b15b50a0efa1.png)\n",
    "    * ![](https://img-blog.csdnimg.cn/99f61ced9f5c4e73a38913746582395d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 4、若按婚姻状况属性来划分，属性婚姻状况有三个可能的取值{married，single，divorced}，分别计算划分后的Gini系数增益。\n",
    "    * ![](https://img-blog.csdnimg.cn/0513170c78b749f6b1aa07216f011e38.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_15,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 5、同理可得年收入Gini\n",
    "    * 对于年收⼊属性为数值型属性，⾸先需要对数据按升序排序，然后从⼩到⼤依次⽤相邻值的中间值作为分隔将样本划分为两组。例如当⾯对年收⼊为60和70这两个值时，我们算得其中间值为65。以中间值65作为分割点求出Gini指数。![](https://img-blog.csdnimg.cn/2f0b4437c39046f88f4b906bd9fa70c9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 6、构建决策树\n",
    "    * ![](https://img-blog.csdnimg.cn/2f3cde4c9b6a4620b504f2dba05e6ab2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、总结决策树的划分算法\n",
    "![](https://img-blog.csdnimg.cn/5125240a74e040c6ad0744a38f7e4bd6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "![](https://img-blog.csdnimg.cn/764736c2bf844001a19e9c778b565ee6.png)\n",
    "\n",
    "### 5.1ID3算法\n",
    "#### 存在的缺点\n",
    "*  ID3算法在选择根节点和各内部节点中的分⽀属性时，采⽤信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。\n",
    "*  ID3算法只能对描述属性为离散型属性的数据集构造决策树。\n",
    "### 5.2C4.5算法\n",
    "#### 改进\n",
    "* (1) ⽤信息增益率来选择属性\n",
    "* (2) 可以处理连续数值型属性\n",
    "* (3)采⽤了⼀种后剪枝⽅法\n",
    "* (4)对于缺失值的处理\n",
    "#### 优点\n",
    "* 产⽣的分类规则易于理解，准确率较⾼。\n",
    "#### 缺点\n",
    "* 在构造树的过程中，需要对数据集进⾏多次的顺序扫描和排序，因⽽导致算法的低效。\n",
    "此外，C4.5只适合于能够驻留于内存的数据集，当**训练集⼤得⽆法在内存容纳时程序⽆法运⾏。**\n",
    "### 5.3决策树变量的两种类型\n",
    "#### 数字型（Numeric）\n",
    "    变量类型是整数或浮点数，如前⾯例⼦中的“年收⼊”。⽤“>=”，“>”,“<”或“<=”作为分割条件（排序后，利⽤已有的分割情况，可以优化分割算法的时间复杂度）。\n",
    "#### 名称型（Nominal）\n",
    "    类似编程语⾔中的枚举类型，变量只能从有限的选项中选取，⽐如前⾯例⼦中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使⽤“=”来分割。\n",
    "### 5.4如何评估分割点的好坏？\n",
    "如果⼀个分割点可以将当前的所有节点分为两类，使得每⼀类都很“纯”，也就是同⼀类的记录较多，那么就是⼀个好分割点。\n",
    "\n",
    "⽐如上⾯的例⼦，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，⾮常“纯”；“否”的节点，可以\n",
    "偿还贷款和⽆法偿还贷款的⼈都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最⼤，所以按\n",
    "照这种⽅法分割。\n",
    "\n",
    "构建决策树采⽤贪⼼算法，只考虑当前纯度差最⼤的情况作为分割点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、cart剪枝\n",
    "### 学习目标\n",
    "* 了解为什么要剪枝\n",
    "* 知道常用的cart剪枝方法\n",
    "\n",
    "### 6.1为什么要剪枝\n",
    "![](https://img-blog.csdnimg.cn/0c7d9d0ef2514840889bb15cb066533b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_18,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 图形描述\n",
    "    * 横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。\n",
    "    * 实线显示的是决策树在训练集上的精度，虚线显示的则是在⼀个独⽴的测试集上测量出来的精度。\n",
    "    * 随着树的增⻓，在训练样集上的精度是单调上升的， 然⽽在独⽴的测试样例上测出的精度先上升后下降。\n",
    "* 出现这种情况的原因\n",
    "    * 原因1：噪声、样本冲突，即错误的样本数据。\n",
    "    * 原因2：特征即属性不能完全作为分类标准。\n",
    "    * 原因3：巧合的规律性，数据量不够⼤。\n",
    "\n",
    "剪枝 (pruning)是决策树学习算法对付\"过拟合\"的主要⼿段。\n",
    "### 6.2剪枝\n",
    "#### 常用的剪枝的方法（判断决策树泛化性能是否提升）\n",
    "* 预剪枝\n",
    "    指**在决策树⽣成过程中，对每个结点在划分前先进⾏估计，若当前结点的划分不能带来决策树泛化性能提升，则停⽌划分并将当前结点标记为叶结点。**      \n",
    "* 后剪枝\n",
    "    先从**训练集⽣成⼀棵完整的决策树，然后⾃底向上地对⾮叶结点进⾏考察**，若将该结点对应的⼦树替换为叶结点能带来决策树泛化性能提升，则将该⼦树替换为叶结点。\n",
    "![](https://img-blog.csdnimg.cn/be8d29c2fbe444868661e85e92b45f30.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_18,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "用信息增益准则来属性划分选择。\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/a5cdd7311ce94db79699f4fbd1e6bf81.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "### 6.3预剪枝\n",
    "\n",
    "* 1、首先\n",
    "\n",
    "基于信息增益准则，我们会选择属性\"脐部\"来对训练集进行划分，产生了3个分支，如下图，然而，在预剪枝中，是否应该选择这个属性进行划分呢？预剪枝要对划分前后的泛化性能进行评估。\n",
    "![](https://img-blog.csdnimg.cn/a5cdd7311ce94db79699f4fbd1e6bf81.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 2、划分之前，所有的样例都集中在根节点。\n",
    "    * 用前面的验证集对“脐部”这个属性的结点决策树进行评估，则编号为 {4，5，8} 的样例被分类正确。另外 4个样例分类错误，于是验证集精度为 ∗ 100% = 42.9%。\n",
    "* 3、在用“脐部”属性进行划分后，上图中的结点2、3、4分别包含编号为 {1，2，3， 14}、 {6，7， 15， 17}、 {10， 16} 的训练样例，因此这 3 个结点分别被标记为叶结点\"好⽠\"、 \"好⽠\"、 \"坏⽠\"。\n",
    "![](https://img-blog.csdnimg.cn/3eae5f6b46474925a508298b5c184141.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "此时，验证集中编号为 {4， 5， 8，11， 12} 的样例被分类正确，验证集精度为 ∗ 100% = 71.4% > 42.9%。于是，⽤\"脐部\"进⾏划分得以确定。\n",
    "\n",
    "* 4、对结点2进行划分\n",
    "\n",
    "基于信息增益准则将挑选出划分属性\"⾊泽\"。然⽽，在使⽤\"⾊泽\"划分后，编号为 {5} 的验证集样本分类结果会由正确转为错误，使得验证集精度下降为 57.1%。于是，**预剪枝策略将禁⽌结点2被划分。**\n",
    "\n",
    "* 5、对结点3进行划分\n",
    "\n",
    "最优划分属性为\"根蒂\"，划分后验证集精度仍为 71.4%. 这个 划分不能提升验证集精度，于是，**预剪枝策略禁⽌结点3被划分。**\n",
    "\n",
    "* 6、综上基于预剪枝策略从上表数据所⽣成的决策树如上图所示，其验证集精度为 71.4%. 这是⼀棵仅有⼀层划分的决策树，亦称\"决策树桩\" (decision stump)。\n",
    "\n",
    "## 6.4后剪枝\n",
    "后剪枝先从训练集⽣成⼀棵完整决策树，继续使⽤上⾯的案例，从前⾯计算，我们知前⾯构造的决策树的验证集精度为42.9%。\n",
    "![](https://img-blog.csdnimg.cn/a5cdd7311ce94db79699f4fbd1e6bf81.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_16,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 1、首先考察结点6\n",
    "\n",
    "若将其领衔的分⽀剪除则相当于把6替换为叶结点。替换后的叶结点包含编号为 {7， 15} 的训练样本，于是该叶结点的类别标记为\"好⽠\"，此时决策树的验证集精度提⾼⾄ 57.1%。于是，**后剪枝策略决定剪枝**，如下图所示。\n",
    "![](https://img-blog.csdnimg.cn/ff239ed7f01d4f3da1111dd491193f70.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZWl6YO95Lya5LiA54K555qE5pys56eR5YOn,size_17,color_FFFFFF,t_70,g_se,x_16)\n",
    "* 2、考察结点5\n",
    "\n",
    "若将其领衔的⼦树替换为叶结点，则替换后的叶结点包含编号为 {6，7，15}的训练样例，叶结点类别标记为\"好⽠'；此时决策树验证集精度仍为 57.1%. 于是，**可以不进⾏剪枝。**\n",
    "\n",
    "* 3、考察结点3\n",
    "\n",
    "对结点2，若将其领衔的⼦树替换为叶结点，则替换后的叶结点包含编号 为 {1， 2， 3， 14} 的训练样例，叶结点标记为\"好⽠\"此时决策树的验证集精度提⾼⾄ 71.4%. 于是，**后剪枝策略决定剪枝。**\n",
    "\n",
    "* 4、考察结点1、3\n",
    "  \n",
    "若将其领衔的⼦树替换为叶结点，则所得决策树的验证集 精度分别为 71.4% 与 42.9%，均未得到提⾼，于是它们被保留。\n",
    "\n",
    "* 5、总结\n",
    "\n",
    "最终，基于后剪枝策略所⽣成的决策树就如上图所示，其验证集精度为 71.4%。\n",
    "\n",
    "## 6.5两种方法对比\n",
    "* 后剪枝决策树通常比预剪枝决策树保留了更多的分支\n",
    "* ⼀般情形下，后剪枝决策树的⽋拟合⻛险很⼩，泛化性能往往优于预剪枝决策树。\n",
    "* 但后剪枝过程是在⽣成完全决策树之后进⾏的。 并且要⾃底向上地对树中的所有⾮叶结点进⾏逐⼀考察，因此其训练时间开销⽐未剪枝决策树和预剪枝决策树都要⼤得多。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
