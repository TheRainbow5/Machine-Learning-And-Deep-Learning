{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "    循环神经网络（Recurrent Neural Networks, RNN） 是一种常用的神经网络结构，它源自于1982年由Saratha Sathasivam提出的霍普菲尔德网络。其特有的循环概念及其最重要的结构——长短时记忆网络——使得它在处理和预测序列数据的问题上有着良好的表现。\n",
    "本文将从如下几方面来具体阐述RNN：\n",
    "* RNN的发展历史\n",
    "* 应用领域\n",
    "* 什么是RNN？\n",
    "* RNN存在的问题\n",
    "* 实现RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN的发展历史\n",
    "\n",
    "1986年，Elman等人提出了用于处理**序列数据**的循环神经网络。如同卷积神经网络是专门用于处理二维数据信息（如图像）的神经网络，循环神经网络是专用于处理序列信息的神经网络。循环网络可以扩展到更长的序列，大多数循环神经网络可以处理可变长度的序列，**循环神经网络的诞生解决了传统神经网络在处理序列信息方面的局限性。**\n",
    "\n",
    "1997年，Hochreiter和Schmidhuber提出了**长短时记忆单元(Long Short-Term Memory, LSTM)** 用于解决标准循环神经网络时间维度的梯度消失问题(vanishing gradient problem)。标准的循环神经网络结构存储的上下文信息的范围有限，限制了RNN的应用。LSTM型RNN用LSTM单元替换标准结构中的神经元节点，LSTM单元使用输入门、输出门和遗忘门控制序列信息的传输，从而实现较大范围的上下文信息的保存与传输。\n",
    "\n",
    "1998年，Williams和Zipser提出名为“**随时间反向传播(Backpropagation Through Time, BPTT)**”的循环神经网络训练算法。BPTT算法的本质是按照时间序列将循环神经网络展开，展开后的网络包含N(时间步长)个隐含单元和一个输出单元，然后采用反向误差传播方式对神经网络的连接权值进行更新。\n",
    "\n",
    "2001年，Gers和Schmidhuber提出了具有重大意义的LSTM型RNN优化模型，在传统的LSTM单元中加入了窥视孔连接(peephole connections)。具有窥视孔连接的LSTM型RNN模型是循环神经网络最流行的模型之一，窥视孔连接进一步提高了LSTM单元对具有长时间间隔相关性特点的序列信息的处理能力。2005年，Graves成功将LSTM型RNN应用于语音处理；2007年，Hochreiter将LSTM型RNN应用于生物信息学研究。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用领域\n",
    "自然语言处理（NLP）\n",
    "* 视频处理\n",
    "* 文本生成\n",
    "* 语言模型\n",
    "* 图像处理\n",
    "\n",
    "机器翻译、语音识别、图像描述生成、文本相似度计算、音乐推荐等领域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是RNN？\n",
    "可以参考视频链接：https://www.bilibili.com/video/BV1z5411f7Bm?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=3040346f6e1d660a222fccde6b153716\n",
    "\n",
    "循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）。\n",
    "\n",
    "RNN是一种特殊的神经网络结构, 它是根据人的认知是基于过往的经验和记忆这一观点提出的. 它与DNN,CNN不同的是: 它不仅考虑前一时刻的输入,而且赋予了网络对前面的内容的一种记忆功能.\n",
    "\n",
    "RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。\n",
    "\n",
    "这是典型的RNN网络在t时刻展开的样子：\n",
    "\n",
    "<img src=\"pic\\9.jpg\" width=500/>\n",
    "\n",
    "<img src=\"pic\\8.jpg\" width=500/>\n",
    "\n",
    "其中：\n",
    "* $x_t$是输入层的输入；\n",
    "* $s_t$是隐藏层的输出，其中$s_{0}$是计算第一个隐藏层所需要的，通常初始化为0；\n",
    "* $o_t$是输出层的输出；\n",
    "\n",
    "从上图可以看出，RNN网络的关键一点是$s_t$的值不仅取决于$x_t$，还取决于$s_{t-1}$\n",
    "\n",
    "假设：\n",
    "* $f$是隐藏层的激活函数，通常是非线性的，如tanh或者ReLU函数\n",
    "* $g$是输出层激活函数，可以是softmax函数。\n",
    "\n",
    "那么，循环神经网络的**前向计算过程**公式如下：\n",
    "$$o_t=g(Vs_{t}+b{2})$$ \n",
    "\n",
    "$$s_{t}=f(Ux_{t}+Ws_{t-1}+b_{1})$$\n",
    "\n",
    "通过俩个公司的循环迭代，有一下推导：\n",
    "\n",
    "<img src=\"pic\\10.jpg\" width=500/>\n",
    "\n",
    "可以看到，当前时刻的输出包含了历史信息，这说明循环神经网络对历史信息进行了保存。\n",
    "\n",
    "这里有几个需要注意的是：\n",
    "* 你可以将隐藏的状态$s_t$看作网络的记忆，它捕获有关所有先前时间步骤中发生的事件的信息。步骤输出$o_t$根据时间t的记忆计算。正如上面简要提到的，它在实践中有点复杂，因为通常无法从太多时间步骤中捕获信息。\n",
    "* 与在每层使用不同参数的传统深度神经网络不同，RNN共享相同的参数（所有步骤的$U,V,W$）。这反映了我们在每个步骤执行相同任务的事实，只是使用不同的输入，这大大减少了我们需要学习的参数总数.\n",
    "* 上图在每个时间步都有输出，但根据任务，这可能不是必需的。例如，在预测句子的情绪时，我们可能只关心最终的输出，而不是每个单词之后的情绪。同样，我们可能不需要在每个时间步骤输入。所以，RNN结构可以是下列不同的组合：\n",
    "<img src=\"pic\\11.jpg\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN存在的问题\n",
    "### 1、梯度爆炸\n",
    "假设我们的时间序列只有三段，再t=3时刻，损失函数为：\n",
    "$$L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^2$$\n",
    "则对于一次训练任务的损失函数为：\n",
    "$$L=\\sum_{t=1}^T L_{t}$$\n",
    "即每一时刻损失之的累加。\n",
    "\n",
    "使用随机梯度下降算法训练RNN其实就是对$W_{x},W_{s},W_{o}$以及$b_{1},b_{2}$求偏导，并不断调整它们使得$L$尽可能达到最小的过程（这里的$U,W,V$参数替换为$W_{x},W_{s},W_{o}$，保持文章的一致。）\n",
    "\n",
    "假设我们的时间序列只有三段$t_{1},t_{2},t_{3}$,我们支队$t_{3}$时刻的$W_{x},W_{s},W_{o}$求偏导（其他时刻类似）:\n",
    "<img src=\"pic\\12.jpg\" width=500/>\n",
    "\n",
    "可以看出损失函数对于$W_{o}$并没有长期依赖，但是因为$s_{t}$随着时间序列向前传播，而$s_{t}$又是$W_{x}、W_{s}$的函数，所以对于$W_{x}、W_{s}$，会随着时间序列产生长期依赖。\n",
    "\n",
    "根据上诉求偏导的过程，我们可以得出任意时刻对$W_{x}、W_{s}$求偏导的公式：\n",
    "\n",
    "$$\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^t \\frac{\\partial{L_{t}}}{\\partial{O_{t}}} \\frac{\\partial{O_{t}}}{\\partial{S_{t}}} (\\prod_{j=k+1}^t \\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}) \\frac{\\partial{S_{k}}}{\\partial{W_{x}}}$$\n",
    "\n",
    "任意时刻对$W_{s}$求偏导的公式同上。\n",
    "\n",
    "如果在隐藏层加上tanh激活函数\n",
    "\n",
    "$$S_{j}=tanh(W_{x}X_{j}+W_{s}S_{j-1}+b_{1})$$\n",
    "\n",
    "这里多了一项$W_{s}S_{j-1}$就是存放着历史信息。则\n",
    "\n",
    "$$\\prod_{j=k+1}^t\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^t tanh'W_{s}$$\n",
    "\n",
    "这个是tanh激活函数图和倒数图：\n",
    "\n",
    "<img src=\"pic\\14.jpg\" width=500/>\n",
    "\n",
    "由上图可知，当激活函数是tanh时，tanh函数的导数（$tanh'$）最大值为1，但是在偏导过程中不可能一直取1，那么就是说，大部分都是小于1的数在做累乘，**如果在$W_{s}$并不是很大的时**，累成数t=100很大时：\n",
    "\n",
    "$$\\prod_{j=k+1}^t\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^t tanh'W_{s}$$\n",
    "\n",
    "$\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}$就会趋向0，这就是梯度消失的原因。\n",
    "\n",
    "同理，当$W_{s}$很大时，$$\\prod_{j=k+1}^t\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^t tanh'W_{s}$$就会趋近于无穷，这是梯度爆炸的原因。\n",
    "\n",
    "## 解决办法\n",
    "RNN的特点本来就是能“追根溯源“利用历史数据，现在告诉我可利用的历史数据竟然是有限的，这就令人非常难受，解决“梯度消失“是非常必要的。解决“梯度消失“的方法主要有： \n",
    "* 选取更好的激活函数 \n",
    "* 改变传播结构\n",
    "* 梯度爆炸：梯度裁剪\n",
    "\n",
    "关于第一点，一般选用ReLU作为激活函数\n",
    "\n",
    "<img src=\"pic\\15.jpg\" width=500/>\n",
    "\n",
    "ReLU函数在定义域大于0部分的导数=1，这样可以解决梯度消失问题。\n",
    "\n",
    "另外计算方便，计算速度快，可以加快网络训练。但是，定义域负数部分恒=0，这样会造成神经元无法激活（可通过合理设置学习率，降低发生的概率）。ReLU有优点也有缺点，其中的缺点可以通过其他操作取避免或者减低发生的概率，是目前使用最多的激活函数。\n",
    "\n",
    "关于第二点，LSTM模型可以很大解决这个问题。\n",
    "\n",
    "关于第三点，如何避免梯度爆炸，我们还是需要关注$\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}$。\n",
    "\n",
    "$$\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^t \\frac{\\partial{L_{t}}}{\\partial{O_{t}}} \\frac{\\partial{O_{t}}}{\\partial{S_{t}}} (\\prod_{j=k+1}^t \\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}) \\frac{\\partial{S_{k}}}{\\partial{W_{x}}}$$\n",
    "\n",
    "梯度消失和爆炸的根本原因就是$\\prod_{j=k+1}^t \\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}$这一部分，要消除这种情况就需要把这一部分在求偏导的过程中去掉，至于怎么去掉，\n",
    "* 一种办法就是使得$ \\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}} \\approx =1$。ReLU激活函数。\n",
    "* 另外一种方法就是使得$ \\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}} \\approx =0$。其实就是LSTM做的事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型介绍\n",
    "### 无隐状态的神经网络\n",
    "类似只有**单隐藏层**的错层感知机，隐变量$S$直接作为输出层的输入，通过自动微积分和随机梯度下降能够学习网络参数就可以了\n",
    "\n",
    "### 有隐状态的神经网络\n",
    "<img src=\"pic\\8.jpg\" width=500/>\n",
    "\n",
    "在任意时间步𝑡，隐状态的计算可以被视为：\n",
    "* 拼接当前时间步𝑡的输入$X_t$和前一时间步$t-1$的隐状态$S_t-1$；\n",
    "* 将拼接的结果送入带有激活函数𝜙的全连接层。 全连接层的输出是当前时间步𝑡的隐状态$S_t$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动手实现RNN模型\n",
    "    RNN模型将在H.G.Wells的时光机器数据集上训练。和前面 :`sec_language_model`中介绍过的一样，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集\n",
    "    首先，我们从H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)中加载文本。这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，而现实中的文档集合可能会包含数十亿个单词。下面的函数(**将数据集读取到由多条文本行组成的列表中**)，其中每条文本行都是一个字符串。为简单起见，我们在这里忽略了标点符号和字母大写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    with open(\"data\\\\timemachine.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    #对文本进行分词处理\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "#词元划分\n",
    "def tokenize(lines,token=\"word\"):\n",
    "    '''将文本行拆分为单词或字符词源'''\n",
    "    if token==\"word\":\n",
    "        return [line.split() for line in lines]\n",
    "    elif token==\"char\":\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print(\"错误，未知词源类型,\"+token)\n",
    "#生成文本词汇表\n",
    "class Vocab():\n",
    "    '''文本词汇表\n",
    "    min_freq：如果一个词出现的频率<min_freq，则放入不认识的token中\n",
    "    reserved_tokens：句子的开头token，或者结尾token\n",
    "    '''\n",
    "    def __init__(self,tokens=None,min_freq=0,reserved_tokens=None):\n",
    "        #如果没有token则为空列表\n",
    "        if tokens is None:\n",
    "            tokens=[]\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens=[]\n",
    "        counter=count_corpus(tokens)  #计算每个token出现的次数（词频统计）\n",
    "        #对词典更具词频进行排序，返回token和freq（词频数）\n",
    "        self.token_freqs=sorted(counter.items(),key=lambda x:x[1],reverse=True)\n",
    "        #unk：位置的token  uniq_tokens：定义token是开头或结尾   \n",
    "        self.unk,uniq_tokens=0,[\"<unk>\"]+reserved_tokens\n",
    "        #只存储词，舍去freq<min_freq的词，并且不是位置词汇\n",
    "        uniq_tokens+=[token for token,freq in self.token_freqs\n",
    "                    if freq >=min_freq and token not in uniq_tokens]\n",
    "        #给你索引转为token保存在列表中，给你token转为索引（不是词频）保存在字典中\n",
    "        self.idx_to_token,self.token_to_idx=[],dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token]=len(self.idx_to_token)-1\n",
    "\n",
    "    #一共有多少个token  \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    #给你token返回index\n",
    "    def __getitem__(self,tokens):\n",
    "        #判断\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            return self.token_to_idx.get(tokens,self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    #给你index，返回token\n",
    "    def to_tokens(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "#统计词源频率       \n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "#词元索引列表和词表\n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]  #返回索引\n",
    "    #如果数据太大，限制大小，加快训练速度\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "#随机抽样\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    #corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    \n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps  #一共可以生成多少子序列\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size   #batche的个数\n",
    "\n",
    "    for i in range(0,num_batches*batch_size, batch_size):\n",
    "        # 每次选出batch_size个随机样本\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        x = [data(j) for j in initial_indices_per_batch]    #序列\n",
    "        y = [data(j + 1) for j in initial_indices_per_batch]  #二元语法\n",
    "        yield torch.tensor(x), torch.tensor(y)\n",
    "#相邻抽样\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        x = Xs[:, i: i + num_steps]\n",
    "        y = Ys[:, i: i + num_steps]\n",
    "        yield x, y \n",
    "#加载数据迭代器\n",
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\n",
    "    max_tokens：如果数据集太大，截断数据集，加快训练\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        #如果是随机采样\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        #相邻采样\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab =load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        #将数据传入\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "#迭代器和词表\n",
    "def load_data_time_machine(batch_size, num_steps,  #@save\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"返回时光机器数据集的迭代器和词表\"\"\"\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "from random import triangular\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "num_step=35            #一个序列的长度\n",
    "\n",
    "#采用相邻抽样\n",
    "train_iter,vocab=load_data_time_machine(batch_size=batch_size, num_steps=num_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独热编码\n",
    "每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。\n",
    "我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为*独热编码*（one-hot encoding），\n",
    "它在`subsec_classification-problem`中介绍过。\n",
    "\n",
    "简言之，将每个索引映射为相互不同的单位向量：\n",
    "假设词表中不同词元的数目为$N$（即`len(vocab)`），\n",
    "词元索引的范围为$0$到$N-1$。\n",
    "如果词元的索引是整数$i$，\n",
    "那么我们将创建一个长度为$N$的全$0$向量，\n",
    "并将第$i$处的元素设置为$1$。\n",
    "此向量是原始词元的一个独热向量。\n",
    "索引为$0$和$2$的独热向量如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.tensor([0,1]),len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们每次采样的(**小批量数据形状是二维张量：（批量大小，时间步数（序列长度））。**)\n",
    "`one_hot`函数将这样一个小批量数据转换成三维张量，\n",
    "张量的最后一个维度等于词表大小（`len(vocab)`）。\n",
    "我们经常转换输入的维度，以便获得形状为\n",
    "**（时间步数，批量大小，词表大小）**的输出。\n",
    "这将使我们能够更方便地通过最外层的维度，\n",
    "一步一步地更新小批量数据的隐状态（隐藏层）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[0, 5],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [3, 8],\n",
      "        [4, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.arange(10).reshape((2, 5))\n",
    "print(x)\n",
    "print(x.T)\n",
    "#print(torch.nn.functional.one_hot(x.T, 28))  #打印结果，观察不同\n",
    "torch.nn.functional.one_hot(x.T, 28).shape  # （时间步数，批量大小，词表大小）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "\n",
    "高级API提供了循环神经网络的实现。\n",
    "我们构造一个具有256个隐藏单元（隐藏层神经元的个数）的单隐藏层的循环神经网络层`rnn_layer`。\n",
    "事实上，我们还没有讨论多层循环神经网络的意义（这将在 :numref:`sec_deep_rnn`中介绍）。\n",
    "现在，你仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer=torch.nn.RNN(len(vocab), num_hiddens)  #rnn_layer只包含隐藏的循环层，后续还需要创建一个单独的输出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们(**使用张量来初始化隐状态**)，它的形状是（隐藏层数，批量大小，隐藏层神经元数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。**]\n",
    "需要强调的是，`rnn_layer`的“输出”（`Y`）不涉及输出层的计算：\n",
    "它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(size=(num_step, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(x, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**我们为一个完整的循环神经网络模型定义了一个`RNNModel`类**]。\n",
    "注意，`rnn_layer`只包含隐藏的循环层，我们还需要创建一个单独的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1870832806.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\jie'jie\\AppData\\Local\\Temp\\ipykernel_12204\\1870832806.py\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self,rnn_layer,vocab_size,**kwargs) -> None:\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn=rnn_layer\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_hiddens=self.rnn.hidden_size   #隐藏层神经元的个数\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            #RNN\n",
    "            self.num_directions = 1  #单向\n",
    "            self.linear = torch.nn.Linear(self.num_hiddens, self.vocab_size)  #将数据拉长\n",
    "        else:\n",
    "            #LSTM\n",
    "            self.num_directions = 2\n",
    "            self.linear = torch.nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "    \n",
    "    def forward(self,inputs,state):\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
