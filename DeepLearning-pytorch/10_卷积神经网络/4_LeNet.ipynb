{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet\n",
    "    它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。这个模型是由AT&T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像 [LeCun et al., 1998]中的手写数字。当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。\n",
    "当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。\n",
    "\n",
    "总体来看，LeNet（LeNet-5）由俩个部分组成：\n",
    "* 卷积编码器（卷积层）：由两个卷积层组成。\n",
    "* 全连接层：由三个全连接层组成。\n",
    "\n",
    "<img src=\"pic\\18.jpg\" width=\"600\"/>\n",
    "\n",
    "每个卷积块中的基本单元是<font color=LightCoral>一个卷积层</font>、<font color=LightCoral>一个sigmoid激活函数</font>和<font color=LightCoral>平均汇聚层</font>。请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用**5x5**卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个**2x2**池操作（步骤2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。\n",
    "\n",
    "为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本。换言之，我们将这个四维输入转换成全连接层所期望的二维输入。**这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示**。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from  torch.utils import data \n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据集\n",
    "    使用Fashion-MNIST图像分类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''下载数据集'''\n",
    "def load_data(batch_size):\n",
    "    # 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，\n",
    "    # 并除以255使得所有像素的数值均在0到1之间\n",
    "    trans = transforms.ToTensor()\n",
    "    #下载训练数据\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"datasets\",  #保存的目录\n",
    "        train=True,       #下载的是训练数据集\n",
    "        transform=trans,   #得到的是pytorch的tensor，而不是图片\n",
    "        download=True)  #从网上下载\n",
    "    #下载测试数据\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"datasets\", train=False, transform=trans, download=True)\n",
    "    len(mnist_train),len(mnist_test)\n",
    "    #装载数据\n",
    "    data_loader_train=data.DataLoader(dataset=mnist_train,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)   #数据是否打乱\n",
    "    data_loader_test=data.DataLoader(dataset=mnist_test,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True)\n",
    "    return data_loader_train,data_loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=torch.nn.Sequential(\n",
    "    #第一层卷积\n",
    "    torch.nn.Conv2d(in_channels=1,  #输入通道\n",
    "                    out_channels=6,  #输出通道\n",
    "                    kernel_size=5,padding=2),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "    #第二层卷积\n",
    "    torch.nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "    #全连接层\n",
    "    torch.nn.Flatten(), \n",
    "    torch.nn.Linear(16*5*5,120),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(120,84),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(84,10)  #最后输出十个\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对原始模型做了一点小改动，去掉了最后一层的**高斯激活**。除此之外，这个网络与最初的LeNet-5一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=torch.rand(size=(1,1,28,28),dtype=torch.float32)\n",
    "#for layer in net:\n",
    "#    x=layer(x)\n",
    "#    print(layer.__class__.__name__,'output shape: \\t',x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义预测准确率函数'''\n",
    "def acc(y_hat,y):\n",
    "    '''\n",
    "    :param y_hat: 接收二维张量，例如 torch.tensor([[1], [0]...])\n",
    "    :param y: 接收二维张量，例如 torch.tensor([[0.1, 0.2, 0.7], [0.8, 0.1, 0.1]...]) 三分类问题\n",
    "    :return:\n",
    "    '''\n",
    "    y_hat=y_hat.argmax(axis=1)\n",
    "    cmp=y_hat.type(y.dtype)==y  #数据类型是否相同\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "    \n",
    "class Accumulator():\n",
    "    ''' 对评估的正确数量和总数进行累加 '''\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "'''自定义每个批次训练函数'''\n",
    "def train_epoch(net,train_iter,loss,optimizer):\n",
    "    #判断是不是pytorch得model，如果是，就打开训练模式，pytorch得训练模式默认开启梯度更新\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.train()\n",
    "    #创建样本累加器【累加每批次的损失值、样本预测正确的个数、样本总数】\n",
    "    metric = Accumulator(3)  \n",
    "    for x,y in train_iter:\n",
    "        #前向传播获取预测结果\n",
    "        y_hat=net(x)\n",
    "        #计算损失\n",
    "        l=loss(y_hat,y) \n",
    "        #判断是pytorch自带得方法还是我们手写得方法（根据不同得方法有不同得处理方式）\n",
    "        if isinstance(optimizer,torch.optim.Optimizer):\n",
    "            #梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            #损失之求和，反向传播（pytorch自动进行了损失值计算）\n",
    "            l.backward()\n",
    "            #更新梯度\n",
    "            optimizer.step()\n",
    "            #累加个参数\n",
    "            metric.add(\n",
    "                float(l)*len(y),  #损失值总数\n",
    "                acc(y_hat,y),     #计算预测正确得总数\n",
    "                y.size().numel()  #样本总数\n",
    "            )\n",
    "    #返回平均损失值，预测正确得概率\n",
    "    return metric[0]/metric[2],metric[1]/metric[2]\n",
    "\n",
    "'''模型测试'''\n",
    "def test_epoch(net,test_iter):\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.eval()  #将模型设置为评估模式\n",
    "    metric=Accumulator(2)\n",
    "    for x,y in test_iter:\n",
    "        metric.add(\n",
    "            acc(net.forward(x),y),  #计算准确个数\n",
    "            y.numel()  #测试样本总数\n",
    "        )\n",
    "    return metric[0]/metric[1]\n",
    "\n",
    "'''正式训练'''\n",
    "def train_LeNet(num_epochs,net,trian_iter,test_iter,lr):\n",
    "    loss_list=[]\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    #初始化权重\n",
    "    def init_weight(m):\n",
    "        if type(m)==torch.nn.Linear or type(m)==torch.nn.Conv2d:\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "    net.apply(init_weight)\n",
    "    #损失函数\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    #优化器\n",
    "    optimizer=torch.optim.SGD(net.parameters(),lr=lr)\n",
    "    #训练\n",
    "    for epoch in range(num_epochs):\n",
    "        #返回平均损失值和正确率\n",
    "        train_metrics=train_epoch(net,trian_iter,loss,optimizer)\n",
    "        loss_list.append(train_metrics[0])  #保存loss\n",
    "        train_acc.append(train_metrics[1])   #保存准确率\n",
    "        #测试集\n",
    "        test_metric=test_epoch(net,test_iter)\n",
    "        test_acc.append(test_metric)\n",
    "        print(f\"epoch{epoch+1}:loss={train_metrics[0]},train_acc={train_metrics[1]*100:.2f}%,test_acc={test_metric*100:.2f}%\")\n",
    "    \n",
    "    return loss_list,train_acc,test_acc\n",
    "\n",
    "'''可视化'''\n",
    "def draw(num_epochs,loss_list,train_acc,test_acc):\n",
    "    fig,ax=plt.subplots()   #定义画布\n",
    "    ax.grid(True)          #添加网格\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    #ax.set_ylim(0,1)\n",
    "\n",
    "    ax.plot(range(num_epochs),loss_list,label=\"loss\")\n",
    "    ax.plot(range(num_epochs),train_acc,dashes=[6, 2],label=\"train\")\n",
    "    ax.plot(range(num_epochs),test_acc,dashes=[6, 2],label=\"test\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1:loss=2.3056928085327146,train_acc=9.83%,test_acc=10.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11556\\1619699542.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mloss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_LeNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11556\\1098797322.py\u001b[0m in \u001b[0;36mtrain_LeNet\u001b[1;34m(num_epochs, net, trian_iter, test_iter, lr)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m#返回平均损失值和正确率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mtrain_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrian_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#保存loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#保存准确率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11556\\1098797322.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(net, train_iter, loss, optimizer)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#前向传播获取预测结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0my_hat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;31m#计算损失\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_epochs=10\n",
    "lr=0.01\n",
    "#数据集\n",
    "train_iter,test_iter=load_data(batch_size=64)\n",
    "#训练\n",
    "loss_list,train_acc,test_acc=train_LeNet(num_epochs,net,train_iter,test_iter,lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化\n",
    "draw(num_epochs,loss_list,train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''删除jpyter缓存'''\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "for root,dirs,files in os.walk('./'):\n",
    "    root_silit=root.split('\\\\')\n",
    "    if '__pycache__' in root_silit or '.ipynb_checkpoints' in root_silit:\n",
    "        print('删除：',root)\n",
    "        shutil.rmtree(root)\n",
    "s=input('操作完成，按任意键退出...')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
