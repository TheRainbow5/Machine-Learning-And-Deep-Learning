{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "    虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。\n",
    "经典卷积神经网络的基本组成部分是下面的这个序列：\n",
    "* 带填充以保持分辨率的卷积层；\n",
    "* 非线性激活函数，如ReLU；\n",
    "* 汇聚层，如最大汇聚层。\n",
    "\n",
    "而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的VGG论文中 [Simonyan & Zisserman, 2014]，\n",
    "\n",
    "## VGG块的组成\n",
    "\n",
    "* 3x3卷积核、填充为1（保持高度和宽度）的卷积层\n",
    "* 2x2汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层。\n",
    "\n",
    "在下面的代码中，我们定义了一个名为vgg_block的函数来实现一个VGG块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from  torch.utils import data \n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "一个VGG块由多个卷积层和一个最大池化层组成。\n",
    "num_convs：一个块的卷积层数\n",
    "in_channels：输入通道\n",
    "out_channel：输出通道\n",
    "'''\n",
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(torch.nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(torch.nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "vgg_block(2,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG网络\n",
    "    与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。\n",
    "<img src=\"pic\\24.jpg\" width=\"500\"/>\n",
    "\n",
    "* VGG神经网络连接图的几个VGG块（在vgg_block函数中定义）。\n",
    "* 其中有超参数变量conv_arch，该变量指定了每个VGG块里**卷积层个数**和**输出通道数**。\n",
    "* 全连接模块则与AlexNet中的相同。\n",
    "\n",
    "原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为**VGG-11**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''块中的卷积层数和输出通道数'''\n",
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG(conv_arch):\n",
    "    conv_blks=[]\n",
    "    in_channels=1    #图片初始通道数\n",
    "    #返回多个sequential（块），保存在conv_blks列表中\n",
    "    for (num_convs,out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs,in_channels,out_channels))\n",
    "        in_channels=out_channels\n",
    "    #构建vgg网络\n",
    "    net=torch.nn.Sequential(\n",
    "        *conv_blks,\n",
    "        torch.nn.Flatten(),  #全连接层\n",
    "        #这里的7需要根据卷积后的像素变化计算得出\n",
    "        torch.nn.Linear(out_channels*7*7,4096),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.5),\n",
    "        torch.nn.Linear(4096,4096),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0,5),\n",
    "        torch.nn.Linear(4096,10)\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size,resize):\n",
    "    # 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，\n",
    "    # 并除以255使得所有像素的数值均在0到1之间\n",
    "    trans =[transforms.ToTensor()]\n",
    "    #修改图片大小\n",
    "    if resize:\n",
    "        trans.insert(0,transforms.Resize(resize)) \n",
    "    trans=transforms.Compose(trans)\n",
    "    #下载训练数据\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"datasets\",  #保存的目录\n",
    "        train=True,       #下载的是训练数据集\n",
    "        transform=trans,   #得到的是pytorch的tensor，而不是图片\n",
    "        download=True)  #从网上下载\n",
    "    #下载测试数据\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"datasets\", train=False, transform=trans, download=True)\n",
    "    print(len(mnist_train),len(mnist_test))\n",
    "    #装载数据\n",
    "    data_loader_train=data.DataLoader(dataset=mnist_train,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)   #数据是否打乱\n",
    "    data_loader_test=data.DataLoader(dataset=mnist_test,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True)\n",
    "    return data_loader_train,data_loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义预测准确率函数'''\n",
    "def acc(y_hat,y):\n",
    "    '''\n",
    "    :param y_hat: 接收二维张量，例如 torch.tensor([[1], [0]...])\n",
    "    :param y: 接收二维张量，例如 torch.tensor([[0.1, 0.2, 0.7], [0.8, 0.1, 0.1]...]) 三分类问题\n",
    "    :return:\n",
    "    '''\n",
    "    y_hat=y_hat.argmax(axis=1)\n",
    "    cmp=y_hat.type(y.dtype)==y  #数据类型是否相同\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "    \n",
    "class Accumulator():\n",
    "    ''' 对评估的正确数量和总数进行累加 '''\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "'''自定义每个批次训练函数'''\n",
    "def train_epoch(net,train_iter,loss,optimizer,device):\n",
    "    #判断是不是pytorch得model，如果是，就打开训练模式，pytorch得训练模式默认开启梯度更新\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.train()\n",
    "    #创建样本累加器【累加每批次的损失值、样本预测正确的个数、样本总数】\n",
    "    metric = Accumulator(3)  \n",
    "    for x,y in train_iter:\n",
    "        x=x.to(device)                            #<----------------------GPU\n",
    "        y=y.to(device)\n",
    "        #前向传播获取预测结果\n",
    "        y_hat=net(x)\n",
    "        #计算损失\n",
    "        l=loss(y_hat,y) \n",
    "        #判断是pytorch自带得方法还是我们手写得方法（根据不同得方法有不同得处理方式）\n",
    "        if isinstance(optimizer,torch.optim.Optimizer):\n",
    "            #梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            #损失之求和，反向传播（pytorch自动进行了损失值计算）\n",
    "            l.backward()\n",
    "            #更新梯度\n",
    "            optimizer.step()\n",
    "            #累加个参数\n",
    "            metric.add(\n",
    "                float(l)*len(y),  #损失值总数\n",
    "                acc(y_hat,y),     #计算预测正确得总数\n",
    "                y.size().numel()  #样本总数\n",
    "            )\n",
    "    #返回平均损失值，预测正确得概率\n",
    "    return metric[0]/metric[2],metric[1]/metric[2]\n",
    "\n",
    "'''模型测试'''\n",
    "def test_epoch(net,test_iter,device):\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.eval()  #将模型设置为评估模式\n",
    "    metric=Accumulator(2)\n",
    "    for x,y in test_iter:\n",
    "        x=x.to(device)                            #<----------------------GPU\n",
    "        y=y.to(device)\n",
    "        metric.add(\n",
    "            acc(net.forward(x),y),  #计算准确个数\n",
    "            y.numel()  #测试样本总数\n",
    "        )\n",
    "    return metric[0]/metric[1]\n",
    "\n",
    "'''正式训练'''\n",
    "def train_LeNet(num_epochs,trian_iter,test_iter,lr,conv_arch):\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    net=VGG(conv_arch)  #VGG网络\n",
    "    net=net.to(device)  #将网络放在gpu或cpu运行<--------------\n",
    "    loss_list=[]\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    #初始化权重\n",
    "    def init_weight(m):\n",
    "        if type(m)==torch.nn.Linear or type(m)==torch.nn.Conv2d:\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "    net.apply(init_weight)\n",
    "    #损失函数\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    #优化器\n",
    "    optimizer=torch.optim.SGD(net.parameters(),lr=lr)\n",
    "    #训练\n",
    "    for epoch in range(num_epochs):\n",
    "        #返回平均损失值和正确率\n",
    "        train_metrics=train_epoch(net,trian_iter,loss,optimizer,device)  #<-----训练\n",
    "        loss_list.append(train_metrics[0])  #保存loss\n",
    "        train_acc.append(train_metrics[1])   #保存准确率\n",
    "        #测试集\n",
    "        test_metric=test_epoch(net,test_iter,device)     #<-------------测试\n",
    "        test_acc.append(test_metric)\n",
    "        print(f\"epoch{epoch+1}:loss={train_metrics[0]},train_acc={train_metrics[1]*100:.2f}%,test_acc={test_metric*100:.2f}%\")\n",
    "    \n",
    "    return loss_list,train_acc,test_acc\n",
    "\n",
    "'''可视化'''\n",
    "def draw(num_epochs,loss_list,train_acc,test_acc):\n",
    "    fig,ax=plt.subplots()   #定义画布\n",
    "    ax.grid(True)          #添加网格\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    #ax.set_ylim(0,1)\n",
    "\n",
    "    ax.plot(range(num_epochs),loss_list,label=\"loss\")\n",
    "    ax.plot(range(num_epochs),train_acc,dashes=[6, 2],label=\"train\")\n",
    "    ax.plot(range(num_epochs),test_acc,dashes=[6, 2],label=\"test\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_epochs=10\n",
    "lr=0.01\n",
    "#定义每个块的参数（卷积层和输出通道）\n",
    "conv_arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "#查看网络参数\n",
    "x=torch.randn(size=(1,1,224,224))\n",
    "for blk in VGG(conv_arch):\n",
    "    x=blk(x)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',x.shape)\n",
    "#数据集\n",
    "train_iter,test_iter=load_data(batch_size=64,resize=224)\n",
    "#训练\n",
    "loss_list,train_acc,test_acc=train_LeNet(num_epochs,train_iter,test_iter,lr,conv_arch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
